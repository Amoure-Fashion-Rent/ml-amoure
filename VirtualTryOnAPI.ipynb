{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRPLgdbG9ckZ"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/OOTDiffusion\n",
        "%cd /content/OOTDiffusion\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/images/demo.png -d /content/OOTDiffusion/images -o demo.png\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/humanparsing/exp-schp-201908261155-lip.pth -d /content/OOTDiffusion/checkpoints/humanparsing -o exp-schp-201908261155-lip.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/humanparsing/exp-schp-201908301523-atr.pth -d /content/OOTDiffusion/checkpoints/humanparsing -o exp-schp-201908301523-atr.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/openpose/ckpts/body_pose_model.pth -d /content/OOTDiffusion/checkpoints/openpose/ckpts -o body_pose_model.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/feature_extractor/preprocessor_config.json -d /content/OOTDiffusion/checkpoints/ootd/feature_extractor -o preprocessor_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm/config.json -d /content/OOTDiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm/diffusion_pytorch_model.safetensors -d /content/OOTDiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_garm -o diffusion_pytorch_model.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/config.json -d /content/OOTDiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton/diffusion_pytorch_model.safetensors -d /content/OOTDiffusion/checkpoints/ootd/ootd_hd/checkpoint-36000/unet_vton -o diffusion_pytorch_model.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/scheduler/scheduler_config.json -d /content/OOTDiffusion/checkpoints/ootd/scheduler -o scheduler_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/text_encoder/config.json -d /content/OOTDiffusion/checkpoints/ootd/text_encoder -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/ootd/text_encoder/pytorch_model.bin -d /content/OOTDiffusion/checkpoints/ootd/text_encoder -o pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/tokenizer/merges.txt -d /content/OOTDiffusion/checkpoints/ootd/tokenizer -o merges.txt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/tokenizer/special_tokens_map.json -d /content/OOTDiffusion/checkpoints/ootd/tokenizer -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/tokenizer/tokenizer_config.json -d /content/OOTDiffusion/checkpoints/ootd/tokenizer -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/tokenizer/vocab.json -d /content/OOTDiffusion/checkpoints/ootd/tokenizer -o vocab.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/vae/config.json -d /content/OOTDiffusion/checkpoints/ootd/vae -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/resolve/main/checkpoints/ootd/vae/diffusion_pytorch_model.bin -d /content/OOTDiffusion/checkpoints/ootd/vae -o diffusion_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/OOTDiffusion/raw/main/checkpoints/ootd/model_index.json -d /content/OOTDiffusion/checkpoints/ootd -o model_index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/SUPIR/resolve/main/clip-vit-large-patch14.tar -d /content/OOTDiffusion/checkpoints -o clip-vit-large-patch14.tar\n",
        "\n",
        "!pip install -q gradio config einops ninja diffusers==0.24.0 accelerate==0.26.1\n",
        "\n",
        "%cd /content/OOTDiffusion/checkpoints\n",
        "!mkdir clip-vit-large-patch14\n",
        "%cd /content/OOTDiffusion/checkpoints/clip-vit-large-patch14\n",
        "!tar -xvf ../clip-vit-large-patch14.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "!pip install fastapi uvicorn pillow torch\n",
        "!pip install tensorflow\n",
        "!npm install -g localtunnel\n",
        "\n",
        "# Navigate to the run directory\n",
        "%cd /content/OOTDiffusion/run"
      ],
      "metadata": {
        "id": "oXzm8PSy-HoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image as PILImage\n",
        "from fastapi import FastAPI, UploadFile, File, Form\n",
        "from fastapi.responses import FileResponse\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import uuid\n",
        "import json\n",
        "import numpy as np\n",
        "import io\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from fastapi import FastAPI, Form\n",
        "from PIL import Image as PILImage\n",
        "from fastapi.responses import FileResponse\n",
        "\n",
        "# Setup project root and import paths\n",
        "PROJECT_ROOT = Path(__file__).absolute().parents[1].absolute()\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from utils_ootd import get_mask_location\n",
        "from preprocess.openpose.run_openpose import OpenPose\n",
        "from preprocess.humanparsing.aigc_run_parsing import Parsing\n",
        "from ootd.inference_ootd_hd import OOTDiffusionHD\n",
        "from ootd.inference_ootd_dc import OOTDiffusionDC\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow CORS (Optional, adjust origins accordingly)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allow all origins\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allow all methods\n",
        "    allow_headers=[\"*\"],  # Allow all headers\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "openpose_model_hd = OpenPose(0)\n",
        "parsing_model_hd = Parsing(0)\n",
        "ootd_model_hd = OOTDiffusionHD(0)\n",
        "\n",
        "openpose_model_dc = OpenPose(0)\n",
        "parsing_model_dc = Parsing(0)\n",
        "ootd_model_dc = OOTDiffusionDC(0)\n",
        "\n",
        "@app.post(\"/process_hd/\")\n",
        "async def process_hd_endpoint(\n",
        "    vton_img: UploadFile = File(...),\n",
        "    garm_img: UploadFile = File(...),\n",
        "    n_samples: int = Form(1),\n",
        "    image_scale: float = Form(2.0),\n",
        "    seed: int = Form(42)\n",
        "):\n",
        "    # Read and process images\n",
        "    vton_image = PILImage.open(vton_img.file).resize((768, 1024))\n",
        "    garm_image = PILImage.open(garm_img.file).resize((768, 1024))\n",
        "\n",
        "    keypoints = openpose_model_hd(vton_image.resize((384, 512)))\n",
        "    model_parse, _ = parsing_model_hd(vton_image.resize((384, 512)))\n",
        "\n",
        "    mask, mask_gray = get_mask_location('hd', 'upper_body', model_parse, keypoints)\n",
        "    mask = mask.resize((768, 1024), PILImage.NEAREST)\n",
        "    mask_gray = mask_gray.resize((768, 1024), PILImage.NEAREST)\n",
        "\n",
        "    masked_vton_img = PILImage.composite(mask_gray, vton_image, mask)\n",
        "\n",
        "    images = ootd_model_hd(\n",
        "        model_type='hd',\n",
        "        category='upperbody',\n",
        "        image_garm=garm_image,\n",
        "        image_vton=masked_vton_img,\n",
        "        mask=mask,\n",
        "        image_ori=vton_image,\n",
        "        num_samples=n_samples,\n",
        "        image_scale=image_scale,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    # Save image to a temporary file\n",
        "    output_image_path = f\"/tmp/output_image_hd_{uuid.uuid4().hex}.png\"\n",
        "    images[0].save(output_image_path)\n",
        "\n",
        "    return FileResponse(output_image_path, media_type=\"image/png\")\n",
        "\n",
        "@app.post(\"/process_dc/\")\n",
        "async def process_dc_endpoint(\n",
        "    vton_url: str = Form(...),\n",
        "    garm_url: str = Form(...),\n",
        "    category: str = Form(...),\n",
        "    n_samples: int = Form(1),\n",
        "    image_scale: float = Form(2.0),\n",
        "    seed: int = Form(42)\n",
        "):\n",
        "    # Fetch and process images from URLs\n",
        "    vton_response = requests.get(vton_url)\n",
        "    garm_response = requests.get(garm_url)\n",
        "    vton_image = PILImage.open(BytesIO(vton_response.content)).resize((768, 1024))\n",
        "    garm_image = PILImage.open(BytesIO(garm_response.content)).resize((768, 1024))\n",
        "\n",
        "    keypoints = openpose_model_dc(vton_image.resize((384, 512)))\n",
        "    model_parse, _ = parsing_model_dc(vton_image.resize((384, 512)))\n",
        "\n",
        "    category_dict = {'Upper-body': 'upper_body', 'Lower-body': 'lower_body', 'Dress': 'dresses'}\n",
        "    category_utils = category_dict.get(category, 'upper_body')\n",
        "\n",
        "    mask, mask_gray = get_mask_location('dc', category_utils, model_parse, keypoints)\n",
        "    mask = mask.resize((768, 1024), PILImage.NEAREST)\n",
        "    mask_gray = mask_gray.resize((768, 1024), PILImage.NEAREST)\n",
        "\n",
        "    masked_vton_img = PILImage.composite(mask_gray, vton_image, mask)\n",
        "\n",
        "    images = ootd_model_dc(\n",
        "        model_type='dc',\n",
        "        category=category_utils,\n",
        "        image_garm=garm_image,\n",
        "        image_vton=masked_vton_img,\n",
        "        mask=mask,\n",
        "        image_ori=vton_image,\n",
        "        num_samples=n_samples,\n",
        "        image_scale=image_scale,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    # Save image to a temporary file\n",
        "    output_image_path = f\"/tmp/output_image_dc_{uuid.uuid4().hex}.png\"\n",
        "    images[0].save(output_image_path)\n",
        "\n",
        "    # Read the file to send it to the backend\n",
        "    with open(output_image_path, \"rb\") as file:\n",
        "        files = {'file': (output_image_path, file, 'image/png')}\n",
        "        response = requests.post(\"https://amoure-backend-awu2hmc2hq-et.a.run.app/ml/image/upload\", files=files)\n",
        "\n",
        "    # Parse the response to get the imageUrl\n",
        "    if response.status_code == 200:\n",
        "        image_url = response.json().get(\"data\", {}).get(\"imageUrl\")\n",
        "        return JSONResponse(content={\"imageUrl\": image_url})\n",
        "    else:\n",
        "        return JSONResponse(content={\"error\": \"Failed to upload image\"}, status_code=500)\n",
        "\n",
        "\n",
        "# app = FastAPI()\n",
        "\n",
        "# @app.post(\"/process_dc/\")\n",
        "# async def process_dc_endpoint(\n",
        "#     vton_url: str = Form(...),\n",
        "#     garm_url: str = Form(...),\n",
        "#     category: str = Form(...),\n",
        "#     n_samples: int = Form(1),\n",
        "#     image_scale: float = Form(2.0),\n",
        "#     seed: int = Form(42)\n",
        "# ):\n",
        "#     # Fetch and process images from URLs\n",
        "#     vton_response = requests.get(vton_url)\n",
        "#     garm_response = requests.get(garm_url)\n",
        "#     vton_image = PILImage.open(BytesIO(vton_response.content)).resize((768, 1024))\n",
        "#     garm_image = PILImage.open(BytesIO(garm_response.content)).resize((768, 1024))\n",
        "\n",
        "#     keypoints = openpose_model_dc(vton_image.resize((384, 512)))\n",
        "#     model_parse, _ = parsing_model_dc(vton_image.resize((384, 512)))\n",
        "\n",
        "#     category_dict = {'Upper-body': 'upper_body', 'Lower-body': 'lower_body', 'Dress': 'dresses'}\n",
        "#     category_utils = category_dict.get(category, 'upper_body')\n",
        "\n",
        "#     mask, mask_gray = get_mask_location('dc', category_utils, model_parse, keypoints)\n",
        "#     mask = mask.resize((768, 1024), PILImage.NEAREST)\n",
        "#     mask_gray = mask_gray.resize((768, 1024), PILImage.NEAREST)\n",
        "\n",
        "#     masked_vton_img = PILImage.composite(mask_gray, vton_image, mask)\n",
        "\n",
        "#     images = ootd_model_dc(\n",
        "#         model_type='dc',\n",
        "#         category=category_utils,\n",
        "#         image_garm=garm_image,\n",
        "#         image_vton=masked_vton_img,\n",
        "#         mask=mask,\n",
        "#         image_ori=vton_image,\n",
        "#         num_samples=n_samples,\n",
        "#         image_scale=image_scale,\n",
        "#         seed=seed,\n",
        "#     )\n",
        "\n",
        "#     # Save image to a temporary file\n",
        "#     output_image_path = f\"/tmp/output_image_dc_{uuid.uuid4().hex}.png\"\n",
        "#     images[0].save(output_image_path)\n",
        "\n",
        "#     return FileResponse(output_image_path, media_type=\"image/png\")\n",
        "\n",
        "# @app.post(\"/process_dc/\")\n",
        "# async def process_dc_endpoint(\n",
        "#     vton_img: UploadFile = File(...),\n",
        "#     garm_img: UploadFile = File(...),\n",
        "#     category: str = Form(...),\n",
        "#     n_samples: int = Form(1),\n",
        "#     image_scale: float = Form(2.0),\n",
        "#     seed: int = Form(42)\n",
        "# ):\n",
        "#     # Read and process images\n",
        "#     vton_image = PILImage.open(vton_img.file).resize((768, 1024))\n",
        "#     garm_image = PILImage.open(garm_img.file).resize((768, 1024))\n",
        "\n",
        "#     keypoints = openpose_model_dc(vton_image.resize((384, 512)))\n",
        "#     model_parse, _ = parsing_model_dc(vton_image.resize((384, 512)))\n",
        "\n",
        "#     category_dict = {'Upper-body': 'upper_body', 'Lower-body': 'lower_body', 'Dress': 'dresses'}\n",
        "#     category_utils = category_dict.get(category, 'upper_body')\n",
        "\n",
        "#     mask, mask_gray = get_mask_location('dc', category_utils, model_parse, keypoints)\n",
        "#     mask = mask.resize((768, 1024), PILImage.NEAREST)\n",
        "#     mask_gray = mask_gray.resize((768, 1024), PILImage.NEAREST)\n",
        "\n",
        "#     masked_vton_img = PILImage.composite(mask_gray, vton_image, mask)\n",
        "\n",
        "#     images = ootd_model_dc(\n",
        "#         model_type='dc',\n",
        "#         category=category_utils,\n",
        "#         image_garm=garm_image,\n",
        "#         image_vton=masked_vton_img,\n",
        "#         mask=mask,\n",
        "#         image_ori=vton_image,\n",
        "#         num_samples=n_samples,\n",
        "#         image_scale=image_scale,\n",
        "#         seed=seed,\n",
        "#     )\n",
        "\n",
        "#     # Save image to a temporary file\n",
        "#     output_image_path = f\"/tmp/output_image_dc_{uuid.uuid4().hex}.png\"\n",
        "#     images[0].save(output_image_path)\n",
        "\n",
        "#     return FileResponse(output_image_path, media_type=\"image/png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True, reload_dirs=[\"/content/OOTDiffusion/run\"])\n"
      ],
      "metadata": {
        "id": "xZ8PuXYY-M7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install nest_asyncio pyngrok\n",
        "# Replace with your actual ngrok auth token\n",
        "NGROK_AUTH_TOKEN = \"2hY8ckzlx4uYyLGd0dRJIPr5lb6_6CVxUyFRXhbTLCyB2Ky9U\"\n",
        "!ngrok config add-authtoken $NGROK_AUTH_TOKEN"
      ],
      "metadata": {
        "id": "xWo8HFwv-SyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the app's location\n",
        "%cd /content/OOTDiffusion/run\n",
        "\n",
        "import subprocess\n",
        "import signal\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Kill any existing Uvicorn processes using the port\n",
        "try:\n",
        "    # Check for existing process using the port and kill it\n",
        "    pid = int(subprocess.check_output([\"lsof\", \"-t\", \"-i:8000\"]).strip())\n",
        "    os.kill(pid, signal.SIGKILL)\n",
        "    print(f\"Killed existing process on port 8000 with PID {pid}\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"No existing process on port 8000\")\n",
        "\n",
        "# Create an ngrok tunnel\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"ngrok tunnel available at: {public_url}\")\n",
        "\n",
        "# Send the public URL to the specified endpoint\n",
        "response = requests.post(\n",
        "    \"https://amoure-backend-awu2hmc2hq-et.a.run.app/ml/ngrok\",\n",
        "    json={\"url\": public_url, \"type\":\"VTON\"}\n",
        ")\n",
        "print(f\"Response from server: {response.status_code} - {response.text}\")\n",
        "\n",
        "# Function to run the FastAPI server\n",
        "def run_uvicorn():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start the Uvicorn server directly\n",
        "run_uvicorn()\n"
      ],
      "metadata": {
        "id": "dopE_iCL-VNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}