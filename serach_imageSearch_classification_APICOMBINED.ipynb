{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obNB324m_BP-"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn pillow torch tensorflow\n",
        "\n",
        "# Optionally, ins tall localtunnel if you want to expose your local server to the internet\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image as PILImage\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from fastapi import FastAPI, HTTPException, Query, UploadFile, File, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from typing import Optional\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import pandas as pd\n",
        "from pinecone_text.sparse import BM25Encoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "# Setup project root and import paths\n",
        "PROJECT_ROOT = Path(__file__).absolute().parents[1].absolute()\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow CORS (Optional, adjust origins accordingly)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Allow all origins\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],  # Allow all methods\n",
        "    allow_headers=[\"*\"],  # Allow all headers\n",
        ")\n",
        "\n",
        "# Setup Pinecone for search\n",
        "pc = Pinecone(api_key='2f1689ff-2b91-4d12-8657-8ccd3c15abb6')\n",
        "\n",
        "# Config for hybrid search\n",
        "index_name_hybrid = \"text-search\"\n",
        "index_name_image = \"image-retrieval\"\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)\n",
        "\n",
        "# Create and connect to hybrid search index\n",
        "if index_name_hybrid not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        index_name_hybrid,\n",
        "        dimension=384,\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    while not pc.describe_index(index_name_hybrid).status['ready']:\n",
        "        time.sleep(1)\n",
        "index_hybrid = pc.Index(index_name_hybrid)\n",
        "\n",
        "# Create and connect to image retrieval index\n",
        "if index_name_image not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        index_name_image,\n",
        "        dimension=512,\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    while not pc.describe_index(index_name_image).status['ready']:\n",
        "        time.sleep(1)\n",
        "index_image = pc.Index(index_name_image)\n",
        "\n",
        "# Setup Sentence Transformer for hybrid search\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_hybrid = SentenceTransformer(\n",
        "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Setup BM25\n",
        "data = pd.read_csv('/content/rtr_combined.csv')\n",
        "data.drop_duplicates()\n",
        "data['title'] = data['title'] + ' ' + data['designer']\n",
        "bm25 = BM25Encoder()\n",
        "bm25.fit(data['title'])\n",
        "\n",
        "# Setup Fashion Clip for image search\n",
        "processor_image = AutoProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
        "model_image = AutoModelForZeroShotImageClassification.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
        "model_image = model_image.to(device)\n",
        "\n",
        "# Load TensorFlow classification model\n",
        "classification_model_path = '/content/model_frozen.h5'\n",
        "classification_model = load_model(classification_model_path)\n",
        "\n",
        "# Load class indices\n",
        "class_indices_path = '/content/class_indices.json'\n",
        "with open(class_indices_path, 'r') as f:\n",
        "    class_indices = json.load(f)\n",
        "    class_labels = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Functions for hybrid search\n",
        "def hybrid_score_norm(dense, sparse, alpha: float):\n",
        "    if alpha < 0 or alpha > 1:\n",
        "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
        "    hs = {\n",
        "        'indices': sparse['indices'],\n",
        "        'values':  [v * (1 - alpha) for v in sparse['values']]\n",
        "    }\n",
        "    return [v * alpha for v in dense], hs\n",
        "\n",
        "def retrieve_by_filter(query, category=None, top_k=20, alpha=0.5):\n",
        "    sparse = bm25.encode_queries(query)\n",
        "    dense = model_hybrid.encode(query).tolist()\n",
        "    hdense, hsparse = hybrid_score_norm(dense, sparse, alpha=alpha)\n",
        "    if category is None:\n",
        "        result = index_hybrid.query(\n",
        "            top_k=top_k,\n",
        "            vector=hdense,\n",
        "            sparse_vector=hsparse,\n",
        "            include_metadata=True\n",
        "        )\n",
        "    else:\n",
        "        result = index_hybrid.query(\n",
        "            top_k=top_k,\n",
        "            vector=hdense,\n",
        "            sparse_vector=hsparse,\n",
        "            filter={\n",
        "                \"category\": {\"$eq\": category}\n",
        "            },\n",
        "            include_metadata=True\n",
        "        )\n",
        "    return [d['id'] for d in result['matches']]\n",
        "\n",
        "# GET API for text-based search\n",
        "@app.get(\"/search/\")\n",
        "async def search(\n",
        "    query: str,\n",
        "    category: Optional[str] = None,\n",
        "    top_k: int = Query(20, ge=1),\n",
        "    alpha: float = Query(0.5, ge=0, le=1)\n",
        "):\n",
        "    try:\n",
        "        results = retrieve_by_filter(\n",
        "            query=query,\n",
        "            category=category,\n",
        "            top_k=top_k,\n",
        "            alpha=alpha\n",
        "        )\n",
        "        return JSONResponse(content={\"result\": results})\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# POST API for image-based search\n",
        "@app.post(\"/image_search/\")\n",
        "async def image_search(\n",
        "    image_url: str = Form(...)\n",
        "):\n",
        "    try:\n",
        "        # Fetch and process the image from the URL\n",
        "        response = requests.get(image_url)\n",
        "        image = PILImage.open(BytesIO(response.content))\n",
        "        inputs = processor_image(images=image, return_tensors=\"pt\").to(device)\n",
        "        image_features = model_image.get_image_features(**inputs)\n",
        "        image_features = image_features.detach().cpu().numpy()[0].tolist()\n",
        "\n",
        "        # Search\n",
        "        result = index_image.query(\n",
        "            top_k=20,\n",
        "            vector=image_features,\n",
        "            include_metadata=True,\n",
        "        )\n",
        "\n",
        "        # Extract only the part before the '_'\n",
        "        ids = [d['id'].split('_')[0] for d in result['matches']]\n",
        "        return JSONResponse(content={\"result\": ids})\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# POST API for image classification\n",
        "@app.post(\"/predict_classification/\")\n",
        "async def predict_classification(\n",
        "    image_url: str = Form(...)\n",
        "):\n",
        "    try:\n",
        "        # Fetch and preprocess the image from the URL\n",
        "        response = requests.get(image_url)\n",
        "        img = PILImage.open(BytesIO(response.content)).resize((224, 224))  # Ensure target size matches model input size\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = preprocess_input(img_array)  # Ensure preprocessing matches training phase\n",
        "\n",
        "        # Make a prediction\n",
        "        predictions = classification_model.predict(img_array)\n",
        "        predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Map the predicted class to its label\n",
        "        predicted_label = class_labels[predicted_class[0]]\n",
        "\n",
        "        return JSONResponse(content={\"predicted_label\": predicted_label})\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# # POST API for image-based search\n",
        "# @app.post(\"/image_search/\")\n",
        "# async def image_search(\n",
        "#     image_file: UploadFile = File(...)\n",
        "# ):\n",
        "#     try:\n",
        "#         image = PILImage.open(image_file.file)\n",
        "#         inputs = processor_image(images=image, return_tensors=\"pt\").to(device)\n",
        "#         image_features = model_image.get_image_features(**inputs)\n",
        "#         image_features = image_features.detach().cpu().numpy()[0].tolist()\n",
        "\n",
        "#         # search\n",
        "#         result = index_image.query(\n",
        "#             top_k=20,\n",
        "#             vector=image_features,\n",
        "#             include_metadata=True,\n",
        "#         )\n",
        "\n",
        "#         # Extract only the part before the '_'\n",
        "#         ids = [d['id'].split('_')[0] for d in result['matches']]\n",
        "#         return JSONResponse(content={\"result\": ids})\n",
        "\n",
        "#     except Exception as e:\n",
        "#         raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# # POST API for image classification\n",
        "# @app.post(\"/predict_classification/\")\n",
        "# async def predict_classification(\n",
        "#     image_file: UploadFile = File(...)\n",
        "# ):\n",
        "#     try:\n",
        "#         # Read and preprocess the image\n",
        "#         img = PILImage.open(image_file.file).resize((224, 224))  # Ensure target size matches model input size\n",
        "#         img_array = image.img_to_array(img)\n",
        "#         img_array = np.expand_dims(img_array, axis=0)\n",
        "#         img_array = preprocess_input(img_array)  # Ensure preprocessing matches training phase\n",
        "\n",
        "#         # Make a prediction\n",
        "#         predictions = classification_model.predict(img_array)\n",
        "#         predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "#         # Map the predicted class to its label\n",
        "#         predicted_label = class_labels[predicted_class[0]]\n",
        "\n",
        "#         return JSONResponse(content={\"predicted_label\": predicted_label})\n",
        "#     except Exception as e:\n",
        "#         raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080, reload=True)\n"
      ],
      "metadata": {
        "id": "MajX0Agh_lma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the app's location\n",
        "# %cd /content/OOTDiffusion/run\n",
        "\n",
        "import subprocess\n",
        "import signal\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Kill any existing Uvicorn processes using the port\n",
        "try:\n",
        "    # Check for existing process using the port and kill it\n",
        "    pid = int(subprocess.check_output([\"lsof\", \"-t\", \"-i:8000\"]).strip())\n",
        "    os.kill(pid, signal.SIGKILL)\n",
        "    print(f\"Killed existing process on port 8000 with PID {pid}\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"No existing process on port 8000\")\n",
        "\n",
        "# Create an ngrok tunnel\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"ngrok tunnel available at: {public_url}\")\n",
        "\n",
        "# Send the public URL to the specified endpoint\n",
        "response = requests.post(\n",
        "    \"https://amoure-backend-awu2hmc2hq-et.a.run.app/ml/ngrok\",\n",
        "    json={\"url\": public_url, \"type\":\"DEFAULT\"}\n",
        ")\n",
        "print(f\"Response from server: {response.status_code} - {response.text}\")\n",
        "\n",
        "# Function to run the FastAPI server\n",
        "def run_uvicorn():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Start the Uvicorn server directly\n",
        "run_uvicorn()\n"
      ],
      "metadata": {
        "id": "gSv-OaBU_wyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}